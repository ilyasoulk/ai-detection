# AI vs Human Text Detection using Intrinsic Dimension

## Project Overview
This project aims to detect whether a given text has been generated by an AI model or written by a human. The detection leverages the concept of intrinsic dimension through three primary methods:

1. **N-gram Rank Analysis**
   - This method analyzes the ranks of n-grams (2-grams, 3-grams, 4-grams) in the text.
   - AI-generated texts often exhibit lower-ranked n-grams due to the constraints of the model's `lm_head` embedding dimension and vocabulary size, which limit the model's capacity to explore diverse combinations of tokens.
   - In contrast, human-written text does not face these constraints, leading to higher-ranked n-grams.

2. **Embedding-based Dimensionality Analysis (PCA, PHD)**
   - This approach uses embedding models (e.g., BERT, GPT) to analyze the text's intrinsic dimensionality.
   - By performing Principal Component Analysis (PCA) on the embeddings, we measure the number of dimensions needed to explain a high variance threshold (95%).
   - AI-generated texts typically exhibit lower intrinsic dimensions compared to human-written texts due to the structured nature of their generation.

3. **RoBERTa-based Classification**
   - A fine-tuned RoBERTa model trained to distinguish between AI and human-written text.
   - Provides high precision for AI text detection and high recall for human text identification.

## Dataset Generation
To create synthetic datasets for training and evaluation, we used the CNN/Daily Mail dataset. The process involved:

1. **Text Truncation:** Randomly truncating human-written articles.
2. **Completion:** Asking various language models to generate completions for the truncated articles.
3. **Labeling:** Retaining and labeling these completions as either human (0) or AI (1).

Different models were used for data generation, and the datasets are named accordingly. You can explore the datasets [here on Hugging Face](https://huggingface.co/collections/zcamz/ai-vs-human-6754d445b3826df8fd547c0e).
## Project Structure

```plaintext
├── config/                 # Model configuration files for text generation
├── prompts/               # System prompts used for text generation
└── src/
    ├── pca.py            # PCA-based intrinsic dimension analysis and evaluation
    ├── ngram.py          # N-gram rank analysis and evaluation
    ├── pretrained.py     # RoBERTa-based classification
    └── dataset/
        ├── generate.py   # Dataset generation pipeline
        └── push_hf.py    # Utility to push datasets to Hugging Face
```

Each component serves a specific purpose:
- `config/`: Contains YAML configuration files for different models used in text generation
- `prompts/`: Stores system prompts that guide the AI models during text generation
- `src/`: Main source code directory
  - `pca.py`: Implements PCA-based intrinsic dimensionality analysis
  - `ngram.py`: Implements n-gram rank analysis method
  - `pretrained.py`: Handles RoBERTa model fine-tuning and evaluation
  - `dataset/`: Dataset management
    - `generate.py`: Orchestrates the dataset generation pipeline
    - `push_hf.py`: Handles dataset upload to Hugging Face Hub

## Results Summary

### N-gram Analysis
| Dataset Name                               | Accuracy | F1-Score (Class 0) | F1-Score (Class 1) |
|-------------------------------------------|----------|--------------------|--------------------|
| `zcamz/ai-vs-human-google-gemma-2-2b-it`  | 71%      | 72%                | 69%                |
| `zcamz/ai-vs-human-Qwen-Qwen2.5-1.5B`     | 69%      | 63%                | 73%                |
| `zcamz/ai-vs-human-HuggingFaceTB-SmolLM2-360M` | 89% | 89%                | 89%                |
| `zcamz/ai-vs-human-HuggingFaceTB-SmolLM2-1.7B` | 75% | 75%                | 76%                |
| `zcamz/ai-vs-human-meta-llama-Llama-3.2-1B` | 64% | 65%                | 63%                |
| `zcamz/ai-vs-human-meta-llama-Llama-3.1-8B` | 60% | 69%                | 42%                |

### Zero-shot Domain Transfer Evaluation
We conducted two types of zero-shot evaluations to test our method's generalization capabilities:

1. **Domain Transfer** (CNN/DailyMail → OpenWebText):
   - Training: Llama-3.1-8B on CNN/DailyMail
   - Testing: Llama-3.1-8B on OpenWebText
   - Results:
     - Accuracy: 58%
     - F1-Score (Human): 0.70
     - F1-Score (AI): 0.32
   - Notable: Very high recall for human text (97%) but poor recall for AI text (19%)

2. **Model Transfer** (Qwen → Llama):
   - Training: Qwen-2.5-1.5B
   - Testing: Llama-3.2-1B
   - Results:
     - Accuracy: 61%
     - F1-Score (Human): 0.58
     - F1-Score (AI): 0.63
   - Notable: More balanced performance between classes

These evaluations demonstrate that:
- Domain transfer is challenging, with a strong bias towards classifying texts as human-written
- Model transfer shows better balanced performance, though still modest
- The method maintains some discriminative power across both domain and model shifts, but with significant performance degradation compared to in-distribution testing

This evaluation demonstrates the method's generalization capabilities across domains and models not seen during rank computation.
However, the results are pretty low showing that our method has a hard time after a domain and model shift.

### PCA-based Analysis
The PCA analysis was performed with the following parameters:
- Number of tokens: 128
- Number of components: 150
- Variance threshold: 95%


| Dataset          | Accuracy  | Human Precision | AI Precision  |
|------------------|-----------|-----------------|---------------|
| Qwen-2.5-1.5B    | 0.7129    | 0.7009         | 0.7263        |
| SmolLM2-360M     | **0.8910**| **0.9565**     | **0.7969**    |
| SmolLM2-1.7B     | 0.8000    | 0.8280         | 0.7757        |
| Gemma-2-2b       | 0.6634    | 0.6410         | 0.6941        |
| Llama-3.2-1B     | 0.5970    | 0.5877         | 0.6092        |
| Llama-3.1-8B     | 0.5931    | 0.5556         | 0.7381        |

The highest values in each column are marked in bold:
- Accuracy: SmolLM2-360M with 0.8910
- Human Precision: SmolLM2-360M with 0.9565
- AI Precision: SmolLM2-360M with 0.7969

Would you like any additional analysis of these metrics?
### PHD-based Analysis
The PHD analysis was performed with the following parameters:
- Number of tokens: 128
- alpha=1.0
- metric="euclidean
- n_reruns=3
- n_points=7
- n_points_min=3
- MIN_SUBSAMPLE = 40
- INTERMEDIATE_POINTS = 7


| Dataset          | Accuracy  | Human Precision | AI Precision  |
|------------------|-----------|-----------------|---------------|
| Qwen-2.5-1.5B    | -         | -              | -             |
| SmolLM2-360M     | 0.6923    | **0.7912**     | 0.5538        |
| SmolLM2-1.7B     | **0.7350**| 0.7667         | **0.7091**    |
| Gemma-2-2b       | 0.5990    | 0.5980         | 0.6000        |
| Llama-3.2-1B     | 0.6070    | 0.6058         | 0.6082        |
| Llama-3.1-8B     | 0.6765    | 0.6768         | 0.6762        |

The highest values in each column are marked in bold:
- Accuracy: SmolLM2-1.7B with 0.7350
- Human Precision: SmolLM2-360M with 0.7912
- AI Precision: SmolLM2-1.7B with 0.7091

### Highlights:
- **Llama-3.1-8B** has the highest overall accuracy (**72.55%**) and the best human precision (**73.68%**).  
- **Qwen-2.5-1.5B** has the highest AI precision (**74.23%**).  

Let me know if you'd like further refinements!

Let me know if you need further details or specific analysis!

### RoBERTa Fine-tuned Model
Results on SmolLM2-1.7B dataset:
- Accuracy: 76%
- F1-Score (Human): 0.75
- F1-Score (AI): 0.78
- Notable high recall for human text (98%) and high precision for AI text (99%)

### Key Insights
- The `HuggingFaceTB-SmolLM2-360M` dataset achieved the highest accuracy of 89% with the n-gram method.
- PCA-based analysis shows consistent performance across different models, with accuracies ranging from 58% to 67%.
- RoBERTa fine-tuning achieves strong results with particularly high precision for AI text detection.
- Zero-shot transfer to new domains remains challenging but shows promising results.

### Detailed Results
<details>
<summary>Click to expand results for `zcamz/ai-vs-human-google-gemma-2-2b-it`</summary>

**Classification Report:**
```
              precision    recall  f1-score   support

           0       0.69      0.76      0.72      1000
           1       0.73      0.65      0.69      1000

    accuracy                           0.71      2000
   macro avg       0.71      0.71      0.71      2000
weighted avg       0.71      0.71      0.71      2000
```
**Confusion Matrix:**
```
                 Predicted
                 Human   AI
Actual Human     759      241
Actual AI        347      653
```

</details>

<details>
<summary>Click to expand results for `zcamz/ai-vs-human-Qwen-Qwen2.5-1.5B`</summary>

**Classification Report:**
```
              precision    recall  f1-score   support

           0       0.77      0.54      0.63      1000
           1       0.64      0.83      0.73      1000

    accuracy                           0.69      2000
   macro avg       0.70      0.69      0.68      2000
weighted avg       0.70      0.69      0.68      2000
```
**Confusion Matrix:**
```
                 Predicted
                 Human   AI
Actual Human     539      461
Actual AI        165      835
```

</details>

<details>
<summary>Click to expand results for `zcamz/ai-vs-human-HuggingFaceTB-SmolLM2-360M`</summary>

**Classification Report:**
```
              precision    recall  f1-score   support

           0       0.90      0.87      0.89      1000
           1       0.88      0.91      0.89      1000

    accuracy                           0.89      2000
   macro avg       0.89      0.89      0.89      2000
weighted avg       0.89      0.89      0.89      2000
```
**Confusion Matrix:**
```
                 Predicted
                 Human   AI
Actual Human     874      126
Actual AI         92      908
```

</details>

<details>
<summary>Click to expand results for `zcamz/ai-vs-human-HuggingFaceTB-SmolLM2-1.7B`</summary>

**Classification Report:**
```
              precision    recall  f1-score   support

           0       0.77      0.73      0.75      1000
           1       0.74      0.78      0.76      1000

    accuracy                           0.75      2000
   macro avg       0.75      0.75      0.75      2000
weighted avg       0.75      0.75      0.75      2000
```
**Confusion Matrix:**
```
                 Predicted
                 Human   AI
Actual Human     731      269
Actual AI        223      777
```

</details>

<details>
<summary>Click to expand results for `zcamz/ai-vs-human-meta-llama-Llama-3.2-1B`</summary>

**Classification Report:**
```
              precision    recall  f1-score   support

           0       0.63      0.66      0.65      1000
           1       0.64      0.62      0.63      1000

    accuracy                           0.64      2000
   macro avg       0.64      0.64      0.64      2000
weighted avg       0.64      0.64      0.64      2000
```
**Confusion Matrix:**
```
                 Predicted
                 Human   AI
Actual Human     658      342
Actual AI        382      618
```

</details>

## Running the Project
### N-gram Analysis
To run the n-gram analysis:
```bash
uv sync
source .venv/bin/activate
python src/ngram.py --dataset_path [DATASETPATH]
```

### Generating Data
1. Create a config file in the `config` directory for your chosen model.
2. Add a line in the `Makefile`:
   ```bash
   python3 ./src/dataset/generate.py ./config/[MODEL_NAME_CONFIG].yaml
   ```
3. Run the following commands:
   ```bash
   make generate_[MODEL_NAME]
   make generate_[MODEL_name]
   ```

### Embedding-based Intrinsic Dimension Analysis
#### 1. Configure Target Datasets
Edit the `Makefile` to specify the datasets to analyze. Example configuration:
```bash
DATASETS = \
    zcamz/ai-vs-human-google-gemma-2-2b-it \
    zcamz/ai-vs-human-HuggingFaceTB-SmolLM2-360M
```

#### 2. Adjust Subset Size and Embedding Model
Define the subset size of the target dataset and the embedding model in the `Makefile`:
```bash
SUBSET_SIZE=512
EMBEDDING_MODEL=google-bert/bert-base-uncased
```
- `SUBSET_SIZE`: Limits the number of samples for the analysis.
- `EMBEDDING_MODEL`: Specifies the embedding model to use.

#### 3. Execute the Analysis Pipeline
Run both PCA and PHD methods in a single step:
```bash
make compute_all_phd_pca
```
#### 4. Individual Analysis Methods
To execute specific analyses:
- **Run PCA only**:
  ```bash
  make compute_all_pca
  ```
- **Run PHD only**:
  ```bash
  make compute_all_phd
  ```
#### 5. Results and Logs
All logs, including confusion matrices and classifier metrics, will be saved in the `logs` directory. 


## Future Directions
- [x] Refine the n-gram analysis.
- [x] Complete and validate the embedding-based dimensionality method.
- [x] Expand datasets to include more diverse models and text styles.

---
**Hugging Face Datasets Collection:** [Link](https://huggingface.co/collections/zcamz/ai-vs-human-6754d445b3826df8fd547c0e)

