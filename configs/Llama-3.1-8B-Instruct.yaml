---
model: meta-llama/Llama-3.2-1B-Instruct
sampling_params:
  temperature: 0.5
  presence_penalty: 0.5
  frequency_penalty: 0.2
  max_tokens: 1000
batch_size: 5000
one_batch: true
prepend_system_role_to_user: False
